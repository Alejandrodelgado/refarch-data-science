{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border: none\" align=\"left\">\n",
    "   <tr style=\"border: none\">\n",
    "      <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>Tutorial: Build, Save and Deploy Model to IBM Watson Machine Learning (WML)</b></th>\n",
    "      <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true\" alt=\"Watson Machine Learning icon\" height=\"40\" width=\"40\"></th>\n",
    "   </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will learn how to build a predictive model with Spark machine learning API (SparkML) and deploy it for scoring in Watson Machine Learning service (WML). \n",
    "\n",
    "This notebook walks you through these steps:\n",
    "- Build a model with SparkML API\n",
    "- Save the model in the WML repository\n",
    "- Create a Deployment in WML\n",
    "- Invoke the deployed model with a Rest Client to test it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analytics use case implemented in this notebook is telco churn. While it's a simple use case, it implements all steps from the CRISP-DM methodolody, which is the recommended best practice for implementing predictive analytics. \n",
    "![CRISP-DM](https://raw.githubusercontent.com/yfphoon/dsx_demo/master/crisp_dm.png)\n",
    "\n",
    "The analytics process starts with defining the business problem and identifying the data that can be used to solve the problem. For Telco churn, we use demographic and historical transaction data. We also know which customers have churned, which is the critical information for building predictive models. In the next step, we use visual APIs for data understanding and complete some data preparation tasks. In a typical analytics project data preparation will include more steps (for example, formatting data or deriving new variables). \n",
    "\n",
    "Once the data is ready, we can build a predictive model. In our example we are using the SparkML Random Forrest classification model. Classification is a statistical technique which assigns a \"class\" to each customer record (for our use case \"churn\" or \"no churn\"). Classification models use historical data to come up with the logic to predict \"class\", this process is called model training. After the model is created, it's usually evaluated using another data set. \n",
    "\n",
    "Finally, if the model's accuracy meets the expectations, it can be deployed for scoring. Scoring is the process of applying the model to a new set of data. For example, when we receive new transactional data, we can score the customer for the risk of churn.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are loading flat files from Object Storage. If a customer has a different data source, the only difference in the notebook will be the code that connects to the data source and loads the data. \n",
    "\n",
    "Important: The following code connects to one of the instructor's Object Storage. We need to replace variables to point to your project's Object Storage.\n",
    "\n",
    "1. Insert a code cell directly under this cell: Select Insert -> Cell Below\n",
    "2. In the menu bar, click on the Connections icon (0 and 1s)\n",
    "3. Under customer.csv select Insert to code -> Insert SparkSession DataFrame\n",
    "4. Replace values for tenant, username and password\n",
    "5. Delete the cell you inserted: click on the Command icon (next to Format dropdown) and select delete cell. \n",
    "6. Test loading data by positioning cursor at the end of the last line of the cell below and clickking the Run (arrow) icon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# @hidden_cell\n",
    "# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "def set_hadoop_config_with_credentials_78e95108d20b4b6eb7f928636070a5c2(name):\n",
    "    \"\"\"This function sets the Hadoop configuration so it is possible to\n",
    "    access data from Bluemix Object Storage using Spark\"\"\"\n",
    "\n",
    "    prefix = 'fs.swift.service.' + name\n",
    "    hconf = sc._jsc.hadoopConfiguration()\n",
    "    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n",
    "    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n",
    "    # Important: change this to your Object Storage value\n",
    "    hconf.set(prefix + '.tenant', '879160f1a1174d2f912f196ac158ffbf')\n",
    "    # Important: change this to your Object Storage value\n",
    "    hconf.set(prefix + '.username', 'e05bff75d3ab4d5ca60b29eabf7d710c')\n",
    "    # Important: change this to your Object Storage value\n",
    "    hconf.set(prefix + '.password', 'MY,ecea)XzgF8!2Z')\n",
    "    hconf.setInt(prefix + '.http.port', 8080)\n",
    "    hconf.set(prefix + '.region', 'dallas')\n",
    "    hconf.setBoolean(prefix + '.public', False)\n",
    "\n",
    "# you can choose any name\n",
    "name = 'keystone'\n",
    "set_hadoop_config_with_credentials_78e95108d20b4b6eb7f928636070a5c2(name)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Customer Information\n",
    "customer = spark.read.format('csv')\\\n",
    "  .options(header='true', inferschema='true')\\\n",
    "  .load('swift://DataScienceUseCase.' + name + '/customer.csv')\n",
    "  \n",
    "#Churn information    \n",
    "customer_churn = spark.read.format('csv')\\\n",
    "  .options(header='true', inferschema='true')\\\n",
    "  .load('swift://DataScienceUseCase.' + name + '/churn.csv')\n",
    "\n",
    "customer.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the first step ran successfully (you saw the output), then continue reviewing the notebook and running each code cell step by step. Note that not every cell has a visual output. The cell is still running if you see a * in the brackets next to the cell. \n",
    "\n",
    "If the first step didn't finish successfully, check with the instructor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Merge Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged=customer.join(customer_churn,customer['ID']==customer_churn['ID']).select(customer['*'],customer_churn['CHURN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Rename some columns\n",
    "This step is to remove spaces from columns names, it's an example of data preparation that you may have to do before creating a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = merged.withColumnRenamed(\"Est Income\", \"EstIncome\").withColumnRenamed(\"Car Owner\",\"CarOwner\")\n",
    "merged.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Data understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Data preparation and data understanding are the most time-consuming tasks in the data mining process. The data scientist needs to review and evaluate the quality of data before modeling.\n",
    "\n",
    "Visualization is one of the ways to reivew data.\n",
    "\n",
    "The Brunel Visualization Language is a highly succinct and novel language that defines interactive data visualizations based on tabular data. The language is well suited for both data scientists and business users. \n",
    "More information about Brunel Visualization: https://github.com/Brunel-Visualization/Brunel/wiki\n",
    "\n",
    "Try Brunel visualization here: http://brunel.mybluemix.net/gallery_app/renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import brunel\n",
    "Merged = merged.toPandas()\n",
    "%brunel data('Merged') bar x(CHURN) y(EstIncome) mean(EstIncome) color(LocalBilltype) stack tooltip(EstIncome) | x(LongDistance) y(Usage) point color(Paymethod) tooltip(LongDistance, Usage) :: width=1100, height=400 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Build the Spark pipeline and the Random Forest model\n",
    "\"Pipeline\" is an API in SparkML that's used for building models.\n",
    "Additional information on SparkML: https://spark.apache.org/docs/2.0.2/ml-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Prepare string variables so that they can be used by the decision tree algorithm\n",
    "stringIndexer1 = StringIndexer(inputCol='Gender', outputCol='GenderEncoded')\n",
    "stringIndexer2 = StringIndexer(inputCol='Status',outputCol='StatusEncoded')\n",
    "stringIndexer3 = StringIndexer(inputCol='CarOwner',outputCol='CarOwnerEncoded')\n",
    "stringIndexer4 = StringIndexer(inputCol='Paymethod',outputCol='PaymethodEncoded')\n",
    "stringIndexer5 = StringIndexer(inputCol='LocalBilltype',outputCol='LocalBilltypeEncoded')\n",
    "stringIndexer6 = StringIndexer(inputCol='LongDistanceBilltype',outputCol='LongDistanceBilltypeEncoded')\n",
    "stringIndexer7 = StringIndexer(inputCol='CHURN', outputCol='label')\n",
    "\n",
    "# Pipelines API requires that input variables are passed in  a vector\n",
    "assembler = VectorAssembler(inputCols=[\"GenderEncoded\", \"StatusEncoded\", \"CarOwnerEncoded\", \"PaymethodEncoded\", \"LocalBilltypeEncoded\", \\\n",
    "                                       \"LongDistanceBilltypeEncoded\", \"Children\", \"EstIncome\", \"Age\", \"LongDistance\", \"International\", \"Local\",\\\n",
    "                                      \"Dropped\",\"Usage\",\"RatePlan\"], outputCol=\"features\")\n",
    "\n",
    "\n",
    "# instantiate the algorithm, take the default settings\n",
    "rf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "#pipeline = Pipeline(stages=[stringIndexer1, stringIndexer2, stringIndexer3, assembler, rf])\n",
    "pipeline = Pipeline(stages=[stringIndexer1,stringIndexer2,stringIndexer3,stringIndexer4,stringIndexer5,stringIndexer6,stringIndexer7, assembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into train and test datasets\n",
    "train, test = merged.randomSplit([80.0,20.0], seed=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build models\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 6: Score the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Precision model1 = {:.2f}.'.format(results.filter(results.label == results.prediction).count() / float(results.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "print 'Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Save Model in WML repository\n",
    "\n",
    "In this section you will store your model in the Watson Machine Learning (WML) repository by using Python client libraries.\n",
    "* <a href=\"https://console.ng.bluemix.net/docs/services/PredictiveModeling/index.html\">WML Documentation</a>\n",
    "* <a href=\"http://watson-ml-api.mybluemix.net/\">WML API</a> \n",
    "<br/>\n",
    "\n",
    "First, you must import client libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from repository.mlrepositoryclient import MLRepositoryClient\n",
    "from repository.mlrepositoryartifact import MLRepositoryArtifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your authentication information from your instance of the Watson Machine Learning service in <a href=\"https://console.ng.bluemix.net/dashboard/apps/\" target=\"_blank\">Bluemix</a> in the next cell. You can find your information on the **Service Credentials** tab of your service instance in Bluemix. \n",
    "\n",
    "<span style=\"color:red\">Replace the service_path and credentials with your own information</span>\n",
    "\n",
    "service_path=[your url]<br/>\n",
    "username=[your username]<br/>\n",
    "password=[your password]<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service_path = 'https://ibm-watson-ml.mybluemix.net'\n",
    "username = 'db7336ae-b258-4b0c-9bd2-57ca9d090f08'\n",
    "password = 'ff129993-058d-472b-bbcb-edf40568b6c8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authorize the repository client to invoke the service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ml_repository_client = MLRepositoryClient(service_path)\n",
    "ml_repository_client.authorize(username, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model artifact (abstraction layer).\n",
    "\n",
    "<b>Tip:</b> The MLRepositoryArtifact method expects a trained model object, training data, and a model name. (It is this model name that is displayed by the Watson Machine Learning service).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_artifact = MLRepositoryArtifact(model, training_data=train, name=\"Predict Customer Churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save pipeline and model artifacts to your Watson Machine Learning instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saved_model = ml_repository_client.models.save(model_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the saved model properties\n",
    "print \"modelType: \" + saved_model.meta.prop(\"modelType\")\n",
    "print \"creationTime: \" + str(saved_model.meta.prop(\"creationTime\"))\n",
    "print \"modelVersionHref: \" + saved_model.meta.prop(\"modelVersionHref\")\n",
    "print \"label: \" + saved_model.meta.prop(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9:  Generate Authorization Token for Invoking the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib3, requests, json\n",
    "\n",
    "headers = urllib3.util.make_headers(basic_auth='{}:{}'.format(username, password))\n",
    "url = '{}/v2/identity/token'.format(service_path)\n",
    "response = requests.get(url, headers=headers)\n",
    "mltoken = json.loads(response.text).get('token')\n",
    "print mltoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9.1 Copy the generated token into your notepad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10:  Go to WML in Bluemix to create a Deployment Endpoint and Test the Deployed model\n",
    "\n",
    "* In your <a href=\"https://console.ng.bluemix.net/dashboard/apps/\" target=\"_blank\">Bluemix</a> dashboard, click into your WML Service and click the **Launch Dashboard** button under Watson Machine Learing.\n",
    "![WML Launch Dashboard](https://raw.githubusercontent.com/yfphoon/dsx_demo/master/WML_Launch_Dashboard.png)\n",
    "\n",
    "<br/>\n",
    "* You should see your deployed model in the **Models** tab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Under *Actions*, click on the 3 ellipses and click ***Create Deployment***.  Give your deployment configuration a unique name, e.g. \"Predict Customer Churn Deply\", accept the defaults and click **Save**.\n",
    "<br/>\n",
    "<br/>\n",
    "* In the *Deployments tab*, under *Actions*, click **View Details**\n",
    "<br/>\n",
    "<br/>\n",
    "* Scoll down to **API Details**, copy the value of the **Scoring Endpoint** into your notepad.  (e.g. https://ibm-watson-ml.mybluemix.net/32768/v2/scoring/2079)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11:  Invoke the model with a REST Client, e.g. https://client.restlet.com/\n",
    "\n",
    "In the REST client interface enter the following information:\n",
    "\n",
    "1. Protocol:  **HTTPS**\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "2. URI: **your scoring endpoint**  (Step 10)\n",
    "<br/>\n",
    "<br/>\n",
    "3. method: **PUT**\n",
    "<br/>\n",
    "<br/>\n",
    "4. Authorization:  **your generated token** (Step 9)\n",
    "<br/>\n",
    "<br/>\n",
    "5. Content Type: **application/JSON**\n",
    "<br/>\n",
    "<br/>\n",
    "6. JSON Body:  **{\"record\":[2095,\"F\",\"M\",2.000000,77551.100000,\"Y\",33.600000,20.530000,0.000000,41.890000,1.000000,\"CC\",\"Budget\",\"Intnl_discount\",62.420000,2.000000]} **\n",
    "<br/>\n",
    "<br/>\n",
    "7. Click **Send*\n",
    "\n",
    "Scroll down to the **RESPONSE** section to see the scored results\n",
    "\n",
    "**Note:** The values in the JSON body correspond to these input fieldnames, and it does not include the label:\n",
    "<br/>\n",
    "ID,Income,AppliedOnline,Residence,YearCurrentAddress,YearsCurrentEmployer,NumberOfCards,CCDebt,Loans,LoanAmount,SalePrice,Location\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You have come to the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you created a model using SparkML API, deployed it in Watson Machine Learning service for online (real time) scoring and tested it using a REST API client. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Created by **Sidney Phoon**\n",
    "<br/>\n",
    "yfphoon@us.ibm.com\n",
    "<br/>\n",
    "April 25, 2017"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}