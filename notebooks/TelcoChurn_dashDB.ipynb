{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border: none\" align=\"left\">\n",
    "   <tr style=\"border: none\">\n",
    "          <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>Lab: Build a model with data stored in a Bluemix dashDB service</b></th>\n",
    "      <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true\" alt=\"Watson Machine Learning icon\" height=\"40\" width=\"40\"></th>\n",
    "   </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Connect to dashDB and load CUSTOMER table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important: Replace dashDB connection information for loading data from Customer and Churn tables prior to running the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required API and instantiate Spark Context\n",
    "from ingest.Connectors import Connectors\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sparkSession = SparkSession.builder.master(\"local\").appName(\"spark session example\").getOrCreate()\n",
    "\n",
    "# IMPORTANT: Replace all values with values in your dashDB instance\n",
    "customerTable = { Connectors.DASHDB.HOST              : 'dashdb-entry-yp-dal09-09.services.dal.bluemix.net',\n",
    "                      Connectors.DASHDB.DATABASE          : 'BLUDB',\n",
    "                      Connectors.DASHDB.USERNAME          : 'dash9737',\n",
    "                      Connectors.DASHDB.PASSWORD          : 'gDO~np@2IKj4',\n",
    "                      Connectors.DASHDB.SOURCE_TABLE_NAME : 'DASH9737.CUSTOMER'}\n",
    "\n",
    "customer = sparkSession.read.format(\"com.ibm.spark.discover\").options(**customerTable).load()\n",
    "customer.printSchema()\n",
    "customer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Connect to dashDB and load CHURN table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Replace all values with values in your dashDB instance\n",
    "churnTable = { Connectors.DASHDB.HOST              : 'dashdb-entry-yp-dal09-09.services.dal.bluemix.net',\n",
    "                      Connectors.DASHDB.DATABASE          : 'BLUDB',\n",
    "                      Connectors.DASHDB.USERNAME          : 'dash9737',\n",
    "                      Connectors.DASHDB.PASSWORD          : 'gDO~np@2IKj4',\n",
    "                      Connectors.DASHDB.SOURCE_TABLE_NAME : 'DASH9737.CHURN'}\n",
    "\n",
    "customer_churn = sparkSession.read.format(\"com.ibm.spark.discover\").options(**churnTable).load()\n",
    "customer_churn.printSchema()\n",
    "customer_churn.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Merge Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged=customer.join(customer_churn,customer['ID']==customer_churn['ID']).select(customer['*'],customer_churn['CHURN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Rename some columns\n",
    "This step is to clean up columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.withColumnRenamed(\"LONGDISTANCE\", \"LONG_DISTANCE\").withColumnRenamed(\"PAYMETHOD\", \"PAY_METHOD\").withColumnRenamed(\"LOCALBILLTYPE\",\"LOCAL_BILL_TYPE\").withColumnRenamed(\"LONGDISTANCEBILLTYPE\",\"LONG_DISTANCE_BILLTYPE\")\n",
    "merged.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Build the Spark pipeline and the Random Forest model\n",
    "\"Pipeline\" is an API in SparkML that's used for building models.\n",
    "Additional information on SparkML: https://spark.apache.org/docs/2.0.2/ml-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Prepare string variables so that they can be used by the decision tree algorithm\n",
    "stringIndexer1 = StringIndexer(inputCol='GENDER', outputCol='GENDER_ENCODED')\n",
    "stringIndexer2 = StringIndexer(inputCol='STATUS',outputCol='STATUS_ENCODED')\n",
    "stringIndexer3 = StringIndexer(inputCol='CAR_OWNER',outputCol='CAR_OWNER_ENCODED')\n",
    "stringIndexer4 = StringIndexer(inputCol='PAY_METHOD',outputCol='PAY_METHOD_ENCODED')\n",
    "stringIndexer5 = StringIndexer(inputCol='LOCAL_BILL_TYPE',outputCol='LOCAL_BILL_TYPE_ENCODED')\n",
    "stringIndexer6 = StringIndexer(inputCol='LONG_DISTANCE_BILLTYPE',outputCol='LONG_DISTANCE_BILLTYPE_ENCODED')\n",
    "stringIndexer7 = StringIndexer(inputCol='CHURN', outputCol='label')\n",
    "\n",
    "# Pipelines API requires that input variables are passed in  a vector\n",
    "assembler = VectorAssembler(inputCols=[\"GENDER_ENCODED\", \"STATUS_ENCODED\", \"CAR_OWNER_ENCODED\", \"PAY_METHOD_ENCODED\", \"LOCAL_BILL_TYPE_ENCODED\", \\\n",
    "                                       \"LONG_DISTANCE_BILLTYPE_ENCODED\", \"CHILDREN\", \"EST_INCOME\", \"AGE\", \"LONG_DISTANCE\", \"INTERNATIONAL\", \"LOCAL\",\\\n",
    "                                      \"DROPPED\",\"USAGE\",\"RATEPLAN\"], outputCol=\"features\")\n",
    "\n",
    "\n",
    "# instantiate the algorithm, take the default settings\n",
    "rf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "#pipeline = Pipeline(stages=[stringIndexer1, stringIndexer2, stringIndexer3, assembler, rf])\n",
    "pipeline = Pipeline(stages=[stringIndexer1,stringIndexer2,stringIndexer3,stringIndexer4,stringIndexer5,stringIndexer6,stringIndexer7, assembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into train and test datasets\n",
    "train, test = merged.randomSplit([0.8,0.2], seed=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build models\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 6: Score the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Precision model1 = {:.2f}.'.format(results.filter(results.label == results.prediction).count() / float(results.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "print 'Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You have come to the end of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Sidney Phoon**\n",
    "<br/>\n",
    "yfphoon@us.ibm.com\n",
    "<br/>\n",
    "April 25, 2017"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
